{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "import jax\n",
    "import pandas as pd\n",
    "import requests\n",
    "from gaussians import *\n",
    "from jax import numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from tueplots import bundles\n",
    "from tueplots.constants.color import rgb\n",
    "\n",
    "cmap_rw = LinearSegmentedColormap.from_list(\n",
    "    \"rw\", [(1, 1, 1), rgb.tue_red], N=1024)\n",
    "cmap_dw = LinearSegmentedColormap.from_list(\n",
    "    \"dw\", [(1, 1, 1), rgb.tue_dark], N=1024)\n",
    "cmap_bw = LinearSegmentedColormap.from_list(\n",
    "    \"bw\", [(1, 1, 1), rgb.tue_blue], N=1024)\n",
    "cmap_gw = LinearSegmentedColormap.from_list(\n",
    "    \"gw\", [(1, 1, 1), rgb.tue_green], N=1024)\n",
    "cmap_bwr = LinearSegmentedColormap.from_list(\n",
    "    \"bwr\", [rgb.tue_blue, (1, 1, 1), rgb.tue_red], N=1024\n",
    ")\n",
    "\n",
    "\n",
    "plt.rcParams.update(bundles.beamer_moml())\n",
    "plt.rcParams.update({\"figure.dpi\": 200})\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process regression example\n",
    "This example is based on the one in the book by [Rasmussen & Williams](https://gaussianprocess.org/gpml/) (Chapter 5.4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load weekly CO$_2$ data directly from NOAA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv\"\n",
    "url = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\"\n",
    "s = requests.get(url).text\n",
    "\n",
    "# df = pd.read_csv(StringIO(s), sep=\",\", header=51, na_values=\"-999.99\").dropna()\n",
    "df = pd.read_csv(StringIO(s), sep=\",\", header=56, na_values=\"-999.99\").dropna()\n",
    "X = jnp.asarray(df[\"decimal date\"])[:, None]\n",
    "Y = jnp.asarray(df[\"average\"])\n",
    "N = len(X)\n",
    "sigma = 0.1\n",
    "print(f\"Using {N} observations ranging from {X[0]} to {X[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = jnp.linspace(1930, 2053, 2000)[:, None]\n",
    "\n",
    "\n",
    "def plot_data():\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(X, Y, label=\"data\", color=rgb.tue_blue)\n",
    "\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"atmospheric CO$_2$ [ppm]\")\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    ax.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(20))\n",
    "    ax.yaxis.set_minor_locator(ticker.MultipleLocator(5))\n",
    "    ax.set_xlim([1930, 2053])\n",
    "    ax.set_ylim([300, 490])\n",
    "    ax.grid(which=\"major\", axis=\"both\")\n",
    "    ax.legend(loc=\"lower right\", framealpha=1, facecolor=\"w\")\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "fig, ax = plot_data()\n",
    "# fig.savefig(\"co2_data.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some first naÃ¯ve approaches:\n",
    "\n",
    "We could do *parametric regression* with linear or polynomial features, i.e. modelling the data as observations of the function $f(x)= w_1 x + w_0 = \\phi(x) w$. This is equivalent to modelling the function with a GP using the *linear kernel*\n",
    "\n",
    "$$ k(a,b) = \\phi(a)^T \\phi(b) = ab + 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we could just do linear regression, of course:\n",
    "def polynomial_features(x, num_features=2):\n",
    "    # output shape: (n_samples, order)\n",
    "    return (\n",
    "        x ** jnp.arange(num_features)\n",
    "        / jnp.exp(jax.scipy.special.gammaln(jnp.arange(num_features) + 1))\n",
    "        / jnp.sqrt(num_features)\n",
    "    )\n",
    "\n",
    "\n",
    "phi = functools.partial(polynomial_features, num_features=2)\n",
    "phi_X = phi(X)\n",
    "F = phi_X.shape[1]\n",
    "\n",
    "prior = Gaussian(mu=jnp.zeros(F), Sigma=100**2 * jnp.eye(F))\n",
    "posterior = prior.condition(phi_X, Y, sigma**2 * jnp.eye(len(X)))\n",
    "\n",
    "fig, ax = plot_data()\n",
    "\n",
    "# plot the posterior\n",
    "posterior_x = phi(x) @ posterior\n",
    "ax.plot(x[:, 0], posterior_x.mu, color=rgb.tue_red, label=\"linear posterior\")\n",
    "ax.fill_between(\n",
    "    x[:, 0],\n",
    "    posterior_x.mu - 2 * posterior_x.std,\n",
    "    posterior_x.mu + 2 * posterior_x.std,\n",
    "    color=rgb.tue_red,\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# plot the posterior samples\n",
    "num_samples = 3\n",
    "key = jax.random.PRNGKey(0)\n",
    "ax.plot(\n",
    "    x[:, 0],\n",
    "    phi(x) @ posterior.sample(key, num_samples=num_samples).T,\n",
    "    color=rgb.tue_red,\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "ax.legend(loc=\"lower right\", framealpha=1, facecolor=\"w\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_kernel(a, b, theta=1.0):\n",
    "    return theta**2 * (jnp.sum(a * b, axis=-1) + 1.0)\n",
    "\n",
    "\n",
    "def zero_mean(x):\n",
    "    return jnp.zeros_like(x[:, 0])\n",
    "\n",
    "\n",
    "prior = GaussianProcess(zero_mean, lambda a,\n",
    "                        b: linear_kernel(a, b, theta=100.0))\n",
    "\n",
    "posterior = prior.condition(Y, X, sigma)\n",
    "\n",
    "fig, ax = plot_data()\n",
    "\n",
    "posterior.plot_shaded(\n",
    "    ax,\n",
    "    x,\n",
    "    color=rgb.tue_red,\n",
    "    mean_kwargs={\"label\": \"GP posterior\"},\n",
    "    std_kwargs={\"alpha\": 0.4, \"cmap\": cmap_rw},\n",
    "    num_samples=2,\n",
    "    rng_key=key,\n",
    ")\n",
    "ax.legend(loc=\"lower right\", framealpha=1, facecolor=\"w\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also do \"textbook\" GP regression, e.g. using the Rational quadratic kernel\n",
    "\n",
    "$$ k(a,b) = \\theta^2 \\left( 1 + \\frac{(a-b)^2}{2\\ell\\alpha} \\right)^{-\\alpha}  $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# or do generic GP regression:\n",
    "def RQ_kernel(a, b, ell=1.0, alpha=1.0, theta=1.0):\n",
    "    return theta**2 * (1 + jnp.sum((a - b) / ell, axis=-1) ** 2 / (2 * alpha)) ** (\n",
    "        -alpha\n",
    "    )\n",
    "\n",
    "\n",
    "# we set the mean to a constant function at the data mean\n",
    "mean_Y = jnp.mean(Y)\n",
    "\n",
    "\n",
    "def constant_mean(x):\n",
    "    return mean_Y * jnp.ones_like(x[:, 0])\n",
    "\n",
    "\n",
    "# instantiate the Gaussian process prior\n",
    "prior = GaussianProcess(\n",
    "    constant_mean, functools.partial(RQ_kernel, ell=1.0, alpha=0.2, theta=3.0)\n",
    ")\n",
    "\n",
    "# condition the prior on the data\n",
    "posterior = prior.condition(Y, X, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_data()\n",
    "\n",
    "prior.plot_shaded(\n",
    "    ax,\n",
    "    x,\n",
    "    color=rgb.tue_dark,\n",
    "    mean_kwargs={\"label\": \"prior\"},\n",
    "    std_kwargs={\"alpha\": 0.4, \"cmap\": cmap_dw},\n",
    "    num_samples=2,\n",
    "    rng_key=key,\n",
    ")\n",
    "\n",
    "\n",
    "posterior.plot_shaded(\n",
    "    ax,\n",
    "    x,\n",
    "    color=rgb.tue_red,\n",
    "    mean_kwargs={\"label\": \"posterior\"},\n",
    "    std_kwargs={\"alpha\": 0.4, \"cmap\": cmap_rw},\n",
    "    num_samples=2,\n",
    "    rng_key=key,\n",
    ")\n",
    "\n",
    "ax.legend(loc=\"lower right\", framealpha=1, facecolor=\"w\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe it's better to model the long-term trend with a _very slowly_ varying function instead of a linear one. Because a linear model can not help but predict continuous growth in CO$_2$, which we hope will not actually happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def long_term_trend_kernel(a, b, theta=1.0, ell=20.0):\n",
    "    return theta**2 * jnp.exp(-jnp.sum((a - b) ** 2, axis=-1) / (2 * ell**2))\n",
    "\n",
    "\n",
    "prior = GaussianProcess(\n",
    "    constant_mean, functools.partial(\n",
    "        long_term_trend_kernel, ell=20.0, theta=10.0)\n",
    ")\n",
    "\n",
    "# condition the prior on the data\n",
    "posterior = prior.condition(Y, X, sigma)\n",
    "\n",
    "fig, ax = plot_data()\n",
    "\n",
    "prior.plot_shaded(\n",
    "    ax,\n",
    "    x,\n",
    "    color=rgb.tue_dark,\n",
    "    mean_kwargs={\"label\": \"prior\"},\n",
    "    std_kwargs={\"alpha\": 0.4, \"cmap\": cmap_dw},\n",
    "    num_samples=5,\n",
    "    rng_key=key,\n",
    ")\n",
    "\n",
    "\n",
    "posterior.plot_shaded(\n",
    "    ax,\n",
    "    x,\n",
    "    color=rgb.tue_red,\n",
    "    mean_kwargs={\"label\": \"posterior\"},\n",
    "    std_kwargs={\"alpha\": 0.4, \"cmap\": cmap_rw},\n",
    "    num_samples=5,\n",
    "    rng_key=key,\n",
    ")\n",
    "\n",
    "ax.legend(loc=\"lower right\", framealpha=1, facecolor=\"w\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a *sum* of two GPs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sum_kernel(\n",
    "    a, b, theta_long=1.0, theta_rq=1.0, ell_long_term=20.0, ell=1.0, alpha=1.0\n",
    "):\n",
    "    return long_term_trend_kernel(\n",
    "        a, b, theta=theta_long, ell=ell_long_term\n",
    "    ) + RQ_kernel(a, b, theta=theta_rq, ell=ell, alpha=alpha)\n",
    "\n",
    "\n",
    "prior = GaussianProcess(\n",
    "    constant_mean,\n",
    "    functools.partial(\n",
    "        sum_kernel,\n",
    "        theta_long=100.0,\n",
    "        theta_rq=5.0,\n",
    "        ell_long_term=50.0,\n",
    "        ell=1.0,\n",
    "        alpha=0.2,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# condition the prior on the data\n",
    "posterior = prior.condition(Y, X, sigma)\n",
    "\n",
    "fig, ax = plot_data()\n",
    "\n",
    "prior.plot_shaded(\n",
    "    ax,\n",
    "    x,\n",
    "    color=rgb.tue_dark,\n",
    "    mean_kwargs={\"label\": \"prior\", \"zorder\": -2},\n",
    "    std_kwargs={\"alpha\": 0.4, \"cmap\": cmap_dw, \"zorder\": -2},\n",
    "    num_samples=5,\n",
    "    rng_key=key,\n",
    ")\n",
    "\n",
    "posterior.plot_shaded(\n",
    "    ax,\n",
    "    x,\n",
    "    color=rgb.tue_red,\n",
    "    yres=1000,\n",
    "    mean_kwargs={\"label\": \"posterior\", \"zorder\": -2},\n",
    "    std_kwargs={\"alpha\": 0.3, \"cmap\": cmap_rw, \"zorder\": -2},\n",
    "    num_samples=5,\n",
    "    rng_key=key,\n",
    ")\n",
    "\n",
    "ax.legend(loc=\"lower right\", framealpha=1, facecolor=\"w\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to model the periodic component. A way to model periodic functions was introduced by [David JC MacKay](http://www.inference.org.uk/mackay/gpB.pdf) in 1998, using an embedding of the circle within the Euclidean plane: Consider the two dimensional transformation $ u(x) = (\\cos(x),\\sin( x))$ of the input $x$. If we use any stationary kernel $k(a,b)$ of the form `k(a,b) = k( ((a-b)**2).sum(axis=-1) )`, then we can apply it to $u(x)$ and use the trigonometric identity $(\\cos(a) - \\cos(b))^2 + (\\sin(a) - \\sin(b))^2 = 4\\sin^2(\\frac{a-b}{2})$ to get a kernel on the circle. For example, with the squared exponential kernel above, we get\n",
    "\n",
    "$$ k(a,b) = \\exp\\left(-\\frac{2 \\sin^2\\left(\\frac{a-b}{2}\\right)^2}{\\ell^2}  \\right) $$\n",
    "\n",
    "We can change the period of the function to $T$ by applying the linear transformation $x \\to \\pi x / T $. In our case, we actually know that the period is $T=1.0$ years, no need to guess.\n",
    "\n",
    "A minor complication is that we don't actually believe the function to be *perfectly* periodic. We can model a \"decay in periodicity\" over time by multiplying the kernel above with another stationary kernel of length-scale $\\ell \\gg T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def periodic_kernel(x, y, period=1.0, ell=1.0, theta=1.0):\n",
    "    return theta**2 * jnp.exp(\n",
    "        -2 * jnp.sin(jnp.pi * jnp.sum(x - y, axis=-1) / period) ** 2 / ell\n",
    "    )\n",
    "\n",
    "\n",
    "prior = GaussianProcess(constant_mean, lambda a, b: periodic_kernel(a, b))\n",
    "\n",
    "decayingprior = GaussianProcess(\n",
    "    constant_mean,\n",
    "    lambda a, b: periodic_kernel(\n",
    "        a, b) * long_term_trend_kernel(a, b, ell=10, theta=1),\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "for p, ax in zip([prior, decayingprior], axs):\n",
    "\n",
    "    p.plot_shaded(\n",
    "        ax,\n",
    "        jnp.linspace(-5, 5, 400)[:, None],\n",
    "        color=rgb.tue_dark,\n",
    "        mean_kwargs={\"label\": \"prior\", \"zorder\": -2},\n",
    "        yrange=(355, 362),\n",
    "        std_kwargs={\"alpha\": 0.3, \"cmap\": cmap_dw, \"zorder\": -2},\n",
    "        num_samples=3,\n",
    "        rng_key=key,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sum_kernel(\n",
    "    x, y, theta_long=1.0, theta_periodic=1.0, ell_long_term=20.0, ell_periodic=1.0\n",
    "):\n",
    "    return long_term_trend_kernel(\n",
    "        x, y, theta=theta_long, ell=ell_long_term\n",
    "    ) + periodic_kernel(x, y, theta=theta_periodic, ell=ell_periodic, period=1.0)\n",
    "\n",
    "\n",
    "prior = GaussianProcess(\n",
    "    constant_mean,\n",
    "    functools.partial(\n",
    "        sum_kernel,\n",
    "        theta_long=100.0,\n",
    "        theta_periodic=5.0,\n",
    "        ell_long_term=50.0,\n",
    "        ell_periodic=1.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# condition the prior on the data\n",
    "posterior = prior.condition(Y, X, sigma)\n",
    "\n",
    "fig, ax = plot_data()\n",
    "\n",
    "prior.plot_shaded(\n",
    "    ax,\n",
    "    x,\n",
    "    color=rgb.tue_dark,\n",
    "    mean_kwargs={\"label\": \"prior\", \"zorder\": -2},\n",
    "    std_kwargs={\"alpha\": 0.4, \"cmap\": cmap_dw, \"zorder\": -2},\n",
    "    num_samples=5,\n",
    "    rng_key=key,\n",
    ")\n",
    "\n",
    "\n",
    "posterior.plot_shaded(\n",
    "    ax,\n",
    "    x,\n",
    "    yres=1000,\n",
    "    color=rgb.tue_red,\n",
    "    mean_kwargs={\"label\": \"posterior\", \"zorder\": -2},\n",
    "    std_kwargs={\"alpha\": 0.2, \"cmap\": cmap_rw, \"zorder\": -2},\n",
    "    num_samples=2,\n",
    "    rng_key=key,\n",
    ")\n",
    "\n",
    "ax.legend(loc=\"lower right\", framealpha=1, facecolor=\"w\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, time to set up a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The first component in the data is a long-term trend, which we model with the very smooth square exponential kernel\n",
    "def long_term_trend_kernel(x, y, theta=100.0, ell=100.0):\n",
    "    return theta**2 * jnp.exp(-jnp.sum((x - y) ** 2, axis=-1) / (2 * ell**2))\n",
    "\n",
    "\n",
    "# Since know that the period is exactly one year, we do not need to expose this parameter.\n",
    "def periodic_kernel(x, y, ell_period=1.0, ell_decay=50.0, theta=1.0):\n",
    "    period = 1.0  # year\n",
    "    return (\n",
    "        theta**2\n",
    "        * jnp.exp(\n",
    "            -2\n",
    "            * jnp.sin(jnp.pi * jnp.sum(x - y, axis=-1) / period) ** 2\n",
    "            / ell_period**2\n",
    "        )\n",
    "        * jnp.exp(-jnp.sum((x - y) ** 2, axis=-1) / (2 * ell_decay**2))\n",
    "    )\n",
    "\n",
    "\n",
    "# the third component in the model is a medium-term trend, which we model with a rational quadratic kernel\n",
    "def mid_term_trend(x, y, ell=1.0, alpha=1.0, theta=1.0):\n",
    "    return theta**2 * (\n",
    "        1 + jnp.sum((x - y) ** 2, axis=-1) / (2 * alpha * ell**2)\n",
    "    ) ** (-alpha)\n",
    "\n",
    "\n",
    "# finally, we account for local wheather phenomena in Hawaii with a \"noise\" term consisting of a squared\n",
    "# exponential kernel with very small length scale, and a white noise term for measurement noise\n",
    "def noise_kernel(x, y, ell=0.1, theta_weather=0.1, theta_measurement=0.1):\n",
    "    return theta_weather**2 * jnp.exp(\n",
    "        -jnp.sum((x - y) ** 2, axis=-1) / (2 * ell**2)\n",
    "    ) + theta_measurement**2 * jnp.all(x == y, axis=-1)\n",
    "\n",
    "\n",
    "def model_kernel(x, y, parameters):\n",
    "    (\n",
    "        theta_long,\n",
    "        ell_long_term,\n",
    "        theta_periodic,\n",
    "        ell_decay_periodic,\n",
    "        ell_periodic,\n",
    "        theta_mid_term,\n",
    "        ell_mid_term,\n",
    "        shape_mid_term,\n",
    "        theta_weather,\n",
    "        ell_weather,\n",
    "        theta_measurement,\n",
    "    ) = parameters\n",
    "    return (\n",
    "        long_term_trend_kernel(x, y, theta=theta_long, ell=ell_long_term)\n",
    "        + periodic_kernel(\n",
    "            x,\n",
    "            y,\n",
    "            theta=theta_periodic,\n",
    "            ell_period=ell_periodic,\n",
    "            ell_decay=ell_decay_periodic,\n",
    "        )\n",
    "        + mid_term_trend(\n",
    "            x, y, theta=theta_mid_term, ell=ell_mid_term, alpha=shape_mid_term\n",
    "        )\n",
    "        + noise_kernel(\n",
    "            x,\n",
    "            y,\n",
    "            ell_weather,\n",
    "            theta_weather=theta_weather,\n",
    "            theta_measurement=theta_measurement,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# initial guesses for the parameters:\n",
    "theta_long = 100.0  # ppm\n",
    "ell_long_term = 100.0  # years\n",
    "theta_periodic = 5.0  # ppm\n",
    "ell_decay_periodic = 50.0  # years\n",
    "ell_periodic = 1.0  # years\n",
    "theta_mid_term = 1.0  # ppm\n",
    "ell_mid_term = 1.0  # years\n",
    "shape_mid_term = 1.0  # unitless\n",
    "theta_weather = 0.1  # ppm\n",
    "ell_weather = 0.1  # years\n",
    "theta_measurement = 0.1  # ppm\n",
    "\n",
    "init_params = jnp.asarray(\n",
    "    [\n",
    "        theta_long,\n",
    "        ell_long_term,\n",
    "        theta_periodic,\n",
    "        ell_decay_periodic,\n",
    "        ell_periodic,\n",
    "        theta_mid_term,\n",
    "        ell_mid_term,\n",
    "        shape_mid_term,\n",
    "        theta_weather,\n",
    "        ell_weather,\n",
    "        theta_measurement,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define the model\n",
    "gp = GaussianProcess(\n",
    "    constant_mean,\n",
    "    functools.partial(model_kernel, parameters=init_params),\n",
    ")\n",
    "\n",
    "# condition the prior on the data\n",
    "gp_posterior = gp.condition(Y, X, sigma)\n",
    "\n",
    "fig, ax = plot_data()\n",
    "gp_posterior.plot_shaded(\n",
    "    ax,\n",
    "    x,\n",
    "    yres=1000,\n",
    "    color=rgb.tue_red,\n",
    "    mean_kwargs={\"label\": \"GP posterior mean\"},\n",
    "    std_kwargs={\"alpha\": 0.2, \"cmap\": cmap_rw},\n",
    "    num_samples=5,\n",
    "    rng_key=key,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is not bad. Let's see if we could pick out the data if we drew from the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = gp(X).sample(key=key, num_samples=5)\n",
    "fig, ax = plot_data()\n",
    "ax.plot(X, samples.T, color=rgb.tue_blue, alpha=1.0)\n",
    "ax.set_ylim([jnp.min(samples[:]), jnp.max(samples[:])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks decent, but let's see if we can do this even better, by"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimiziation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import grad, hessian\n",
    "from scipy import optimize\n",
    "\n",
    "\n",
    "def NegEvidence(params):\n",
    "    gp = GaussianProcess(\n",
    "        constant_mean,\n",
    "        functools.partial(model_kernel, parameters=params),\n",
    "    )\n",
    "    return -gp(X).log_pdf(Y)\n",
    "\n",
    "\n",
    "grad_neg_ev = grad(NegEvidence)\n",
    "# hess_neg_ev = hessian(NegEvidence)\n",
    "\n",
    "params = init_params\n",
    "\n",
    "# init_negevidence = NegEvidence(params)\n",
    "# grad_init = grad_neg_ev(params)\n",
    "# print(init_negevidence, grad_init)\n",
    "\n",
    "results = optimize.minimize(\n",
    "    NegEvidence,\n",
    "    params,\n",
    "    method=\"CG\",\n",
    "    jac=grad(NegEvidence),\n",
    "    options={\"gtol\": 1e-6, \"disp\": True, \"maxiter\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # an alternative choice of parameters was found in Rasmussen & Williams (2006):\n",
    "# theta_long = 66.0  # ppm\n",
    "# ell_long_term = 67.0  # years\n",
    "# theta_periodic = 2.4  # ppm\n",
    "# ell_decay_periodic = 90.0  # years\n",
    "# ell_periodic = 1.3  # years\n",
    "# theta_mid_term = 0.66  # ppm\n",
    "# ell_mid_term = 1.2  # years\n",
    "# shape_mid_term = 0.78  # unitless\n",
    "# theta_weather = 0.18  # ppm\n",
    "# ell_weather = 0.13  # years\n",
    "# theta_measurement = 0.19  # ppm\n",
    "\n",
    "RW_params = [66.0, 67.0, 2.4, 90.0, 1.3, 0.66, 1.2, 0.78, 0.18, 0.13, 0.19]\n",
    "units = [\n",
    "    \"ppm\",\n",
    "    \"years\",\n",
    "    \"ppm\",\n",
    "    \"years\",\n",
    "    \"years\",\n",
    "    \"ppm\",\n",
    "    \"years\",\n",
    "    \"\",\n",
    "    \"ppm\",\n",
    "    \"years\",\n",
    "    \"ppm\",\n",
    "]\n",
    "\n",
    "names = [\n",
    "    \"theta_long\",\n",
    "    \"ell_long_term\",\n",
    "    \"theta_periodic\",\n",
    "    \"ell_decay_periodic\",\n",
    "    \" ell_periodic\",\n",
    "    \"theta_mid_term\",\n",
    "    \"ell_mid_term\",\n",
    "    \"shape_mid_term\",\n",
    "    \"theta_weather\",\n",
    "    \"ell_weather\",\n",
    "    \"theta_measurement\",\n",
    "]\n",
    "\n",
    "for p, name, r, i, u in zip(results.x, names, RW_params, init_params, units):\n",
    "    print(\n",
    "        name.rjust(25)\n",
    "        + f\"{i:9.2f}\"\n",
    "        + \"  ->  \"\n",
    "        + f\"{p:6.2f}\"\n",
    "        + 5 * \" \"\n",
    "        + \"Ref: \"\n",
    "        + f\"{r:6.2f}\"\n",
    "        + \" \"\n",
    "        + u\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt_params = jnp.asarray(results.x)\n",
    "\n",
    "gp = GaussianProcess(\n",
    "    constant_mean,\n",
    "    functools.partial(model_kernel, parameters=opt_params),\n",
    ")\n",
    "gp_posterior = gp.condition(Y, X, sigma)\n",
    "\n",
    "\n",
    "samples = gp(X).sample(key=key, num_samples=5)\n",
    "fig, ax = plot_data()\n",
    "ax.plot(X, samples.T, color=rgb.tue_blue, alpha=1.0)\n",
    "ax.set_ylim([jnp.min(samples[:]), jnp.max(samples[:])])\n",
    "ax.set_title(\"can you pick out the data?\")\n",
    "\n",
    "\n",
    "fig, ax = plot_data()\n",
    "gp_posterior.plot_shaded(\n",
    "    ax,\n",
    "    x,\n",
    "    yres=1000,\n",
    "    color=rgb.tue_red,\n",
    "    mean_kwargs={\"label\": \"GP posterior mean\"},\n",
    "    std_kwargs={\"alpha\": 0.2, \"cmap\": cmap_rw},\n",
    "    num_samples=5,\n",
    "    rng_key=key,\n",
    ")\n",
    "ax.set_title(\"prediction\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Separation\n",
    "\n",
    "One neat aspect of this GP model is that we can also consider the marginal distributions of individual components in the sum model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first, the long-term component:\n",
    "k_long = functools.partial(\n",
    "    long_term_trend_kernel,\n",
    "    theta=opt_params[0],\n",
    "    ell=opt_params[1],\n",
    ")\n",
    "\n",
    "long_term_GP = gp_posterior.project(k_long)\n",
    "\n",
    "# the medium-term component:\n",
    "k_mid = functools.partial(\n",
    "    mid_term_trend, theta=opt_params[5], ell=opt_params[6], alpha=opt_params[7]\n",
    ")\n",
    "\n",
    "mid_term_GP = gp_posterior.project(k_mid, lambda x: jnp.zeros_like(x[..., 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_data()\n",
    "long_term_GP.plot_shaded(\n",
    "    ax,\n",
    "    x,\n",
    "    color=rgb.tue_red,\n",
    "    mean_kwargs={\"label\": \"long term prediction posterior mean\"},\n",
    "    std_kwargs={\"alpha\": 0.3, \"cmap\": cmap_rw},\n",
    "    num_samples=5,\n",
    "    rng_key=key,\n",
    ")\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "# ax2.yaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "ax2.set_ylim([-5, 5])\n",
    "ax2.yaxis.label.set_color(rgb.tue_green)\n",
    "ax2.spines[\"right\"].set_color(rgb.tue_green)\n",
    "ax2.tick_params(axis=\"y\", colors=rgb.tue_green)\n",
    "ax2.set_ylabel(\"medium term deviation [ppm]\")\n",
    "ax.grid(False)\n",
    "\n",
    "mid_term_GP.plot_shaded(\n",
    "    ax2,\n",
    "    x,\n",
    "    color=rgb.tue_green,\n",
    "    mean_kwargs={\"label\": \"mid term prediction posterior mean\"},\n",
    "    std_kwargs={\"alpha\": 0.3, \"cmap\": cmap_gw},\n",
    "    num_samples=5,\n",
    "    rng_key=key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# and the periodic component:\n",
    "k_periodic = functools.partial(\n",
    "    periodic_kernel,\n",
    "    ell_period=opt_params[4],\n",
    "    ell_decay=opt_params[3],\n",
    "    theta=opt_params[2],\n",
    ")\n",
    "\n",
    "periodic_GP = gp_posterior.project(\n",
    "    k_periodic, lambda x: jnp.zeros_like(x[..., 0]))\n",
    "\n",
    "res = 20\n",
    "x_period = np.zeros((80 * 12 * res, 1))\n",
    "for y in range(80):\n",
    "    for m in range(12 * res):\n",
    "        x_period[y * 12 * res + m] = 1960 + y + m / (12 * res)\n",
    "\n",
    "m_period = (periodic_GP(x_period).mu)[:, None]\n",
    "s_period = (periodic_GP(x_period).std)[:, None]\n",
    "s_period = s_period - np.min(s_period)\n",
    "s_period = s_period / np.max(s_period)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(\n",
    "    m_period.reshape(80, 12 * res),\n",
    "    aspect=\"auto\",\n",
    "    alpha=1 - s_period.reshape(80, 12 * res),\n",
    "    cmap=cmap_bwr,\n",
    "    origin=\"lower\",\n",
    "    extent=[0, 12, 1960, 2040],\n",
    ")\n",
    "cbar = fig.colorbar(im)\n",
    "cbar.set_label(\"periodic component [ppm]\")\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "ax.yaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Year\")\n",
    "ax.grid(axis=\"both\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
