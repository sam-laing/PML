{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning\n",
    "<div style=\"text-align: right\"> University of Tübingen, Summer Term 2023  &copy; 2023 P. Hennig </div>\n",
    "\n",
    "## Exercise Sheet No. 4 — Gaussians\n",
    "\n",
    "---\n",
    "\n",
    "Submission by:\n",
    "* Sam, Laing, Matrikelnummer: 6283670\n",
    "* Albert, Catalan Tatjer, Matrikelnummer:6443478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "key = random.PRNGKey(0)\n",
    "from exponential_families import *\n",
    "from tueplots import bundles\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update(bundles.beamer_moml())\n",
    "plt.rcParams.update({'figure.dpi': 200})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2 (Coding Exercise)\n",
    "\n",
    "> This week's Exercise is directly taken from Exercise 24.3. of [David JC MacKay](https://en.wikipedia.org/wiki/David_J._C._MacKay)'s [*Information Theory, Inference, and Learning Algorithms*](http://www.inference.org.uk/mackay/itila/). (But don't waste your time trying to find an answer there :)\n",
    "\n",
    "The terribly important quantity $\\mu$ has been experimentally measured by seven scientists (A, B, C, D, E, F, G) with wildly differing experimental skills. They have reported the following measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jnp.array([-27.020,3.570,8.191,9.898,9.603,9.945,10.056])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'measured value of $x$')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Generic family 'cursive' not found because none of the following families were found: Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1102.36x496.063 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(X,random.uniform(key, shape=(7,)),'x',ms=3)\n",
    "\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_xlabel('measured value of $x$')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that they have all, independently of each other, made an unbiased Gaussian measurement of $\\mu$: \n",
    "$$p(\\boldsymbol{x}\\mid \\mu,\\boldsymbol{\\sigma}) = \\prod_{i=1} ^7 \\mathcal{N}(x_i;\\mu,\\sigma_i ^2).$$ \n",
    "But we have to assume that their measurement errors $\\sigma_i$ vary a lot (some are skilled experimentalists, others are unqualified). \n",
    "\n",
    "#### Task A: \n",
    "Implement the likelihood above as a single jax function (this is unfortunately a case where our neat `ExponentialFamily` base class is more awkward than useful). Try using a numerical optimizer to find *maximum likelihood estimators*, i.e. points $(\\mu, \\vec{\\sigma})$ that maximize this function. Alternatively, you can try and identify such points directly by inspecting the likelihood by hand.\n",
    "  \n",
    "You probably agree that, intuitively, it looks pretty certain that A and B are both inept measurers, that D–G are better, and that the true value of $\\mu$ is somewhere close to $10$. Are your findings consistent with this intuition?  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Likelihood here is given by\n",
    "$$\n",
    "L(\\mu, \\sigma_1,...,\\sigma_7) = \\prod_{j=1}^{7} {\\frac{1}{\\sqrt{2\\pi}\\sigma_j} \\ \\exp(-\\frac{1}{2} \\frac{(x_j - \\mu)^2}{\\sigma_j^2})}\n",
    "$$\n",
    "So the Log-likelihood is therefore given by:\n",
    "$$\n",
    "\\log \\ L = -7 \\log(\\sqrt{2\\pi}) - \\sum_{j=1}^7 \\log(\\sigma_j) - \\frac{1}{2} \\sum_{j=1}^7 \\frac{(x_j - \\mu)^2}{2\\sigma_j^2} \n",
    "$$\n",
    "We implement this function in jax and then optimize by minimizing the negative log likelihood over $(\\mu, \\sigma_1,...,\\sigma_7)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.optimize import minimize\n",
    "\n",
    "params_init = jnp.array([10, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "\n",
    "# defining the negative log likelihood \n",
    "# (including the square root of a square to ensure positive values for the standard deviations)\n",
    "# also leaving out the constant since it doesn't affect the optimization\n",
    "def neg_log_like(params):\n",
    "    mu, sig = params[0], params[1:]\n",
    "    return jnp.sum(jnp.log(jnp.sqrt(sig)) ** 2 + 0.5 * ((X - mu) / sig) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimizing using jax and initializing with higher variance for samples far away from\n",
    "result = minimize(\n",
    "    neg_log_like, x0=jnp.array([10, 40, 15, 2, 0.5, 0.5, 0.5, 0.25]), method=\"BFGS\"\n",
    ")\n",
    "\n",
    "# Extract the optimized parameters and the minimum value\n",
    "params_optimized = result.x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Parameters: [Array(9.722158, dtype=float32), Array(5.3295555, dtype=float32), Array(2.5276077, dtype=float32), Array(1.5314531, dtype=float32), Array(1.0280777, dtype=float32), Array(1.0135379, dtype=float32), Array(1.0428718, dtype=float32), Array(1.084028, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "p_optim = [params_optimized[0]]\n",
    "p_optim.extend(jnp.sqrt(s) for s in params_optimized[1:])\n",
    "print(\"Optimized Parameters:\", p_optim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our findings seem to agree with $\\mu$ being close to 10 and the variance of the leftmost and second leftmost observers being much higher than the ones close to 10.\n",
    "However, in order to arrive at the \"correct\" local minima, we had to initialize the variance $\\sigma_1$ to be fairly high which is reasonable since otherwise we may find other local minima in this 8 dimensional function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task B:\n",
    "We will now instead provide a Bayesian answer. Let the prior on each $\\sigma_i^{-2}$ be a broad [Gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution), i.e. the distribution\n",
    "$$ \\mathcal{G}(z;\\alpha,\\beta)=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} z^{\\alpha-1} e^{-\\beta z},$$\n",
    "and\n",
    "$$p(\\boldsymbol{\\sigma}) = \\prod_{i=1} ^7 \\mathcal{G}(\\sigma_i^{-2};\\alpha,\\beta),$$ \n",
    "with, say, $\\alpha=1$, $\\beta=0.1$. \n",
    "\n",
    "Let the prior for $\\mu$ be a broad Gaussian $p(\\mu) = \\mathcal{N}(\\mu; m, v^2)$ with mean $m = 0$ and standard deviation $v = 10^3$. \n",
    "\n",
    "Find the posterior for $\\mu$. Plot this posterior for $\\mu$, both for the data given above and for $\\boldsymbol{x} = \\{13.01, 7.39\\}$.\n",
    "\n",
    "\n",
    "**Hint:** First, remember that the Gamma is the conjugate prior for the Gaussian with fixed mean $\\mu$. The marginal $p(\\boldsymbol{x}\\mid\\mu)$ can thus be computed using the `log_marginal_pdf` function you implemented generically for exponential families last week, and which has an analytic form. Then use Bayes' theorem a second time to find $p(\\mu\\mid\\boldsymbol{x})$ up to normalization, by directly multiplying the prior for $\\mu$ and the marginal likelihood terms you just found. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that we have:\n",
    "$$\n",
    "\\int_0^{\\infty} z^{a-1}e^{-bz} dz = \\frac{\\Gamma(a)}{b^a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import functools\n",
    "\n",
    "\n",
    "class ExponentialFamily(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def sufficient_statistics(self, x: ArrayLike, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D)->(P)`\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def log_base_measure(self, x: ArrayLike, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D)->()`\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def log_partition(self, parameters: ArrayLike, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(P)->()`\"\"\"\n",
    "\n",
    "    def parameters_to_natural_parameters(self, parameters: ArrayLike, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(P)->(P)`\n",
    "        In some EF's, the canonical parameters are\n",
    "        actually a transformation of the natural parameters.\n",
    "        In such cases, this method should be overwritten to\n",
    "        provide the inverse transformation.\n",
    "        \"\"\"\n",
    "        return jnp.asarray(parameters)\n",
    "\n",
    "    def logpdf(self, x: ArrayLike, parameters: ArrayLike, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D),(P)->()`\n",
    "        log p(x|parameters)\n",
    "            = log h(x) + sufficient_statistics(x) @ natural_parameters - log Z(natural_parameters)\n",
    "            = log base measure + linear term - log partition\n",
    "        \"\"\"\n",
    "\n",
    "        x = jnp.asarray(x)\n",
    "        log_base_measure = self.log_base_measure(x)\n",
    "        natural_parameters = self.parameters_to_natural_parameters(parameters)\n",
    "        linear_term = (\n",
    "            self.sufficient_statistics(x)[..., None, :] @ natural_parameters[..., None]\n",
    "        )[..., 0, 0]\n",
    "        log_partition = self.log_partition(parameters)\n",
    "\n",
    "        return log_base_measure + linear_term - log_partition\n",
    "\n",
    "    def conjugate_log_partition(\n",
    "        self, alpha: ArrayLike, nu: ArrayLike, /\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"The log partition function of the conjugate exponential family.\n",
    "        Signature `(P),()->()`\n",
    "        If(!) this is available, it allows analytic construction of the conjugate prior (and thus analytic posterior inference).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def conjugate_prior(self) -> \"ConjugateFamily\":\n",
    "        return ConjugateFamily(self)\n",
    "\n",
    "    def predictive_log_marginal_pdf(\n",
    "        self,\n",
    "        x: ArrayLike,\n",
    "        conjugate_natural_parameters: ArrayLike,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D),(P)->()`\n",
    "        log p(x|conjugate_natural_parameters)\n",
    "        Your answer to Part B below should be implemented here.\n",
    "        \"\"\"\n",
    "        x = jnp.asarray(x)\n",
    "        alpha, nu = conjugate_natural_parameters\n",
    "        P = (\n",
    "            self.log_base_measure(x)\n",
    "            + self.conjugate_log_partition(\n",
    "                self.sufficient_statistics(x) + alpha, nu + 1\n",
    "            )\n",
    "            - self.conjugate_log_partition(alpha, nu)\n",
    "        )\n",
    "\n",
    "        return P[0][0], P[1][0]\n",
    "\n",
    "\n",
    "    def posterior_parameters(\n",
    "        self,\n",
    "        prior_natural_parameters: ArrayLike,\n",
    "        data: ArrayLike,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Computes the natural parameters of the posterior distribution under the\n",
    "        conjugate prior.\n",
    "        Signature `(P),(D)->(P)`\n",
    "        This can be implemented already in the abc and inherited by all subclasses,\n",
    "        even if the conjugate log partition function is not available.\n",
    "        (In the latter case, only the unnormalized posterior is immediately available, see below).\n",
    "        \"\"\"\n",
    "\n",
    "        prior_natural_parameters = jnp.asarray(prior_natural_parameters)\n",
    "\n",
    "        sufficient_statistics = self.sufficient_statistics(data)\n",
    "\n",
    "        n = sufficient_statistics[..., 0].size\n",
    "        expected_sufficient_statistics = jnp.sum(\n",
    "            sufficient_statistics,\n",
    "            axis=tuple(range(sufficient_statistics.ndim)),\n",
    "        )\n",
    "\n",
    "        alpha_prior, nu_prior = (\n",
    "            prior_natural_parameters[:-1],\n",
    "            prior_natural_parameters[-1],\n",
    "        )\n",
    "\n",
    "        return jnp.append(alpha_prior + expected_sufficient_statistics, nu_prior + n)\n",
    "\n",
    "    def Laplace_predictive_log_marginal_pdf(self, x, natural_parameters, mode):\n",
    "        def log_posterier(mode):\n",
    "            return self.conjugate_prior().unnormalized_logpdf(mode, natural_parameters)\n",
    "\n",
    "        mode = jnp.asarray(mode)\n",
    "        x = jnp.asarray(x)\n",
    "        alpha, nu = natural_parameters\n",
    "\n",
    "        psi = jax.hessian(log_posterier)(mode)\n",
    "        const = jnp.sqrt((2 * jnp.pi) ** len(mode) * jnp.linalg.det(jnp.linalg.inv(-psi)))\n",
    "\n",
    "        def approx_conj_log_partition(natural_parameters):\n",
    "            alpha, nu = natural_parameters\n",
    "            exp_term = jnp.exp(mode.T * alpha - self.log_partition(mode).T * nu)\n",
    "            return jnp.log(const * exp_term)\n",
    "\n",
    "        L = (\n",
    "            self.log_base_measure(x)\n",
    "            + approx_conj_log_partition(\n",
    "                (self.sufficient_statistics(x) + alpha, nu + 1)\n",
    "            )\n",
    "            - approx_conj_log_partition((alpha, nu))\n",
    "        )\n",
    "        return L[0][0], L[1][0]\n",
    "\n",
    "\n",
    "class ConjugateFamily(ExponentialFamily):\n",
    "    def __init__(self, likelihood: ExponentialFamily) -> None:\n",
    "        self._likelihood = likelihood\n",
    "\n",
    "    @functools.partial(jnp.vectorize, excluded={0}, signature=\"(d)->(p)\")\n",
    "    def sufficient_statistics(self, w: ArrayLike, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D)->(P)`\n",
    "        the sufficient statistics of the conjugate family are\n",
    "        the natural parameters and the (negative) log partition function of the likelihood.\n",
    "        \"\"\"\n",
    "        return jnp.append(\n",
    "            self._likelihood.parameters_to_natural_parameters(w),\n",
    "            -self._likelihood.log_partition(w),\n",
    "        )\n",
    "\n",
    "    def log_base_measure(self, w: ArrayLike, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D)->()`\n",
    "        the base measure of the conjugate family is, implicitly, the Lebesgue measure.\n",
    "        \"\"\"\n",
    "        w = jnp.asarray(w)\n",
    "\n",
    "        return jnp.zeros_like(w[..., 0])\n",
    "\n",
    "    def log_partition(self, natural_parameters: ArrayLike, /) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(P)->()`\n",
    "        If the conjugate log partition function is available,\n",
    "        we can use it to compute the log partition function of the conjugate family.\n",
    "        \"\"\"\n",
    "        natural_parameters = jnp.asarray(natural_parameters)\n",
    "\n",
    "        alpha, nu = natural_parameters[:-1], natural_parameters[-1]\n",
    "\n",
    "        return self._likelihood.conjugate_log_partition(alpha, nu)\n",
    "\n",
    "    def unnormalized_logpdf(\n",
    "        self, w: ArrayLike, natural_parameters: ArrayLike, /\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(D),(P)->()`\n",
    "        Even if the conjugate log partition function is not available,\n",
    "        we can still compute the unnormalized log pdf of the conjugate family.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.sufficient_statistics(w) @ jnp.asarray(natural_parameters)\n",
    "\n",
    "    def laplace_precision(\n",
    "        self,\n",
    "        natural_parameters: ArrayLike,\n",
    "        mode: ArrayLike,\n",
    "        /,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Signature `(P),(D)->()`\n",
    "        If the conjugate log partition function is _not_ available,\n",
    "        we can still compute the Laplace approximation to the posterior,\n",
    "        using only structure provided by the likelihood.\n",
    "        This requires the mode of the likelihood, which is not available in general,\n",
    "        but may be found by numerical optimization if necessary.\n",
    "        \"\"\"\n",
    "        return -jax.hessian(self.unnormalized_logpdf, argnums=0)(\n",
    "            jnp.asarray(mode), natural_parameters\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import gammaln"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our likelihood is Gaussian with mean $\\mu$ and precision $\\sigma^{-2}$ where $\\sigma^{-2} \\sim \\text{Gamma}(a,b)$. We therefore define an exponential family of a Gaussian. Then, as per last week's homework, call the conjugate prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(ExponentialFamily):\n",
    "    def __init__(self, mu):\n",
    "        super().__init__()\n",
    "        # need a fixed mean mu\n",
    "        self.mu = mu\n",
    "\n",
    "    def sufficient_statistics(self, x: ArrayLike | jnp.ndarray) -> jnp.ndarray:\n",
    "        x = jnp.asarray(x)\n",
    "        return jnp.asarray([self.mu * x - 0.5 * x**2])\n",
    "\n",
    "    def log_base_measure(self, x: ArrayLike | jnp.ndarray) -> jnp.ndarray:\n",
    "        x = jnp.asarray(x)\n",
    "        return 0\n",
    "\n",
    "    def log_partition(self, tau: ArrayLike | jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"natural parameters are just w = sig^-2 since mean is fixed\"\"\"\n",
    "        tau = jnp.asarray(tau)\n",
    "        return self.mu**2 - jnp.log(tau) + jnp.log(2*jnp.pi)\n",
    "\n",
    "    def parameters_to_natural_parameters(\n",
    "        self, tau: ArrayLike | jnp.ndarray\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"param  = sig^-2\"\"\"\n",
    "        params = jnp.asarray(params)\n",
    "        return tau\n",
    "\n",
    "    # let's skip this at first glance:\n",
    "    \n",
    "    def conjugate_log_partition(\n",
    "        self, alpha: ArrayLike | jnp.ndarray, nu: ArrayLike | jnp.ndarray\n",
    "    ) -> jnp.ndarray:\n",
    "        \n",
    "        return jax.scipy.integrate( lambda tau: alpha * tau - nu * self.log_partition(tau),0,1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(mu):\n",
    " return Gaussian(mu)\n",
    "\n",
    "def prior(mu):\n",
    "    return likelihood(mu).conjugate_prior()\n",
    "    \n",
    "\n",
    "a,b = 1, 0.1\n",
    "prior_natural_parameters = [a,b]\n",
    "data = X\n",
    "\n",
    "def posterior_nat_params(mu, posterior_nat_params, data):\n",
    "    posterior = prior(mu)\n",
    "    posterior_natural_parameters = likelihood(mu).posterior_parameters(prior_natural_parameters = prior_natural_parameters, data = data)\n",
    "    return posterior_natural_parameters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([7.008911, 1.1     ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior_nat_params(25, [1,0.1], X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior(10).predictive_log_marginal_pdf(x = [[0],[1]], conjugate_natural_parameters=posterior_nat_params(10, [1,0.1], X)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
